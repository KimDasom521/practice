{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nrZ0wvYOwnu"
      },
      "source": [
        "#Set up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7HKe7cms5_s",
        "outputId": "d6a1587e-507c-489a-9c65-8e51d809db0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvbwKi86OJbY",
        "outputId": "4282d60b-db35-47fe-f8ff-c42b2a0aef72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpylE9-dOzDB",
        "outputId": "413bdb25-20e9-48b7-d85e-d614ca48e553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-scatter==2.1.2\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n",
            "Collecting shap\n",
            "  Downloading shap-0.45.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.4)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.0)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.45.1 slicer-0.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter==2.1.2 -f https://pytorch-geometric.com/whl/torch-2.2.1+cu121.html\n",
        "!pip install torch-geometric\n",
        "!pip install shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPwHsMvyOn7N"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AcgFdmPSOi4b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
        "from torch.nn import TransformerEncoderLayer, TransformerEncoder\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch_scatter import scatter_mean\n",
        "# from torch_geometric.nn import MetaLayer\n",
        "\n",
        "\n",
        "TIME_WINDOW = 24\n",
        "PRED_LEN = 6\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, mode, encoder, w_init, w, x_em, date_em, loc_em, edge_h, gnn_h, gnn_layer, city_num, group_num, pred_step, device):\n",
        "        super(Model, self).__init__()\n",
        "        self.device = device\n",
        "        self.mode = mode\n",
        "        self.encoder = encoder\n",
        "        self.w_init = w_init\n",
        "        self.x_em = x_em\n",
        "        self.city_num = city_num\n",
        "        self.group_num = group_num\n",
        "        self.edge_h = edge_h\n",
        "        self.gnn_layer = gnn_layer\n",
        "        self.pred_step = pred_step\n",
        "\n",
        "        if self.encoder == 'self':\n",
        "            self.encoder_layer = TransformerEncoderLayer(8, nhead=4, dim_feedforward=256)\n",
        "            self.x_embed = Lin(TIME_WINDOW*8, x_em)\n",
        "\n",
        "        elif self.encoder == 'lstm':\n",
        "            self.input_LSTM = nn.LSTM(8,x_em,num_layers=1,batch_first=True)\n",
        "\n",
        "\n",
        "        if self.w_init == 'rand':\n",
        "            self.w = Parameter(torch.randn(city_num,group_num).to(device,non_blocking=True),requires_grad=True)\n",
        "\n",
        "        elif self.w_init == 'group':\n",
        "            self.w = Parameter(w,requires_grad=True)\n",
        "\n",
        "\n",
        "        self.loc_embed = Lin(2, loc_em)\n",
        "\n",
        "        self.u_embed1 = nn.Embedding(12, date_em) #month\n",
        "        self.u_embed2 = nn.Embedding(7, date_em) #week\n",
        "        self.u_embed3 = nn.Embedding(24, date_em) #hour\n",
        "\n",
        "        self.edge_inf = Seq(Lin(x_em*2+date_em*3+loc_em*2,edge_h),ReLU(inplace=False))\n",
        "\n",
        "        self.group_gnn = nn.ModuleList([NodeModel(x_em+loc_em,edge_h,gnn_h)])\n",
        "\n",
        "        for i in range(self.gnn_layer-1):\n",
        "            self.group_gnn.append(NodeModel(gnn_h,edge_h,gnn_h))\n",
        "\n",
        "        self.global_gnn = nn.ModuleList([NodeModel(x_em+gnn_h,1,gnn_h)])\n",
        "\n",
        "        for i in range(self.gnn_layer-1):\n",
        "            self.global_gnn.append(NodeModel(gnn_h,1,gnn_h))\n",
        "\n",
        "        if self.mode == 'feedback':\n",
        "            self.predMLP = Seq(Lin(gnn_h,16),ReLU(inplace=False),Lin(16,1),ReLU(inplace=False))\n",
        "\n",
        "        if self.mode == 'probe_x' or self.mode == 'probe_encoded' or self.mode == 'probe_woAttn':\n",
        "            self.predMLP = Seq(Lin(gnn_h,16),ReLU(inplace=False),Lin(16,self.pred_step),ReLU(inplace=False))\n",
        "            self.probe_gnn = nn.ModuleList([NodeModel(x_em,1,gnn_h)])\n",
        "\n",
        "            for i in range(self.gnn_layer-1):\n",
        "                self.probe_gnn.append(NodeModel(gnn_h,1,gnn_h))\n",
        "\n",
        "        if self.mode == 'temp':\n",
        "            self.decoder = DecoderModule(x_em,edge_h,gnn_h,gnn_layer,city_num,group_num,device)\n",
        "            self.predMLP = Seq(Lin(gnn_h,16),ReLU(inplace=False),Lin(16,self.pred_step),ReLU(inplace=False))\n",
        "            self.TemporalAggregateMLP = Seq(Lin(gnn_h+8,gnn_h),ReLU(inplace=False))\n",
        "\n",
        "        if self.mode == 'both':\n",
        "            self.predMLP = Seq(Lin(gnn_h,16),ReLU(inplace=False),Lin(16,1),ReLU(inplace=False))\n",
        "            self.TemporalAggregateMLP = Seq(Lin(gnn_h+8,gnn_h),ReLU(inplace=False))\n",
        "\n",
        "        if self.mode == 'feedbackDecoder':\n",
        "            self.decoder = DecoderModule(x_em,edge_h,gnn_h,gnn_layer,city_num,group_num,device)\n",
        "            self.predMLP = Seq(Lin(gnn_h,16),ReLU(inplace=False),Lin(16,1),ReLU(inplace=False))\n",
        "\n",
        "        if self.mode == 'baseline':\n",
        "            self.decoder = DecoderModule(x_em,edge_h,gnn_h,gnn_layer,city_num,group_num,device)\n",
        "            self.predMLP = Seq(Lin(gnn_h,16),ReLU(inplace=False),Lin(16,self.pred_step),ReLU(inplace=False))\n",
        "\n",
        "        if self.mode == 'final':\n",
        "            self.x_embed = Lin(TIME_WINDOW*8, x_em)\n",
        "\n",
        "            self.predMLP = Seq(Lin(gnn_h,16),ReLU(inplace=False),Lin(16,self.pred_step),ReLU(inplace=False))\n",
        "            self.global_gnn = nn.ModuleList([NodeModel(x_em,1,gnn_h)])\n",
        "\n",
        "            for i in range(self.gnn_layer-1):\n",
        "                self.global_gnn.append(NodeModel(gnn_h,1,gnn_h))\n",
        "\n",
        "        elif self.mode == 'final2':\n",
        "            self.encoder_layer = TransformerEncoderLayer(x_em, nhead=4, dim_feedforward=256, batch_first=True)\n",
        "            self.x_embed = Lin(TIME_WINDOW*8, x_em)\n",
        "\n",
        "            self.predMLP = Seq(Lin(gnn_h,16),ReLU(inplace=False),Lin(16,self.pred_step),ReLU(inplace=False))\n",
        "            self.global_gnn = nn.ModuleList([NodeModel(x_em,1,gnn_h)])\n",
        "\n",
        "            for i in range(self.gnn_layer-1):\n",
        "                self.global_gnn.append(NodeModel(gnn_h,1,gnn_h))\n",
        "\n",
        "\n",
        "    def batchInput(self, x, edge_w, edge_index):\n",
        "        # new_x\n",
        "        sta_num = x.shape[1]\n",
        "        x = x.reshape(-1, x.shape[-1])\n",
        "\n",
        "        # edge_w\n",
        "        edge_w = edge_w.reshape(-1,edge_w.shape[-1])\n",
        "\n",
        "        # edge_index\n",
        "        for i in range(edge_index.size(0)):\n",
        "            edge_index[i,:] = torch.add(edge_index[i,:], i*sta_num)\n",
        "        edge_index = edge_index.transpose(0,1)\n",
        "        edge_index = edge_index.reshape(2,-1)\n",
        "\n",
        "        return x, edge_w, edge_index\n",
        "\n",
        "\n",
        "    def forward(self, x, u, edge_index, edge_w, loc):\n",
        "        # Shape: (batch, 209, 24, 8)\n",
        "        if self.mode == 'final':\n",
        "            x = x.reshape(-1,self.city_num, TIME_WINDOW*x.shape[-1]) # Shape: (batch, 209, 24*8)\n",
        "            x = self.x_embed(x) # Linear(24*8, x_em:32) Shape: (batch, 209, 32)\n",
        "\n",
        "            \"\"\" Update \"\"\"\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            new_x, edge_w, edge_index = self.batchInput(x, edge_w, edge_index)\n",
        "            # new_x Shape: (batch, 209, 64) -> (batch*209, 64)\n",
        "            # edge_w Shape: (batch, 4112) -> (batch*4112, 1)\n",
        "            # edge_index Shape: (batch, 2, 4112) -> (2, batch*4112)\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.global_gnn[i](new_x, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "\n",
        "            \"\"\" Final Forcasting \"\"\"\n",
        "            res = self.predMLP(new_x)\n",
        "            res = res.reshape(-1,self.city_num,self.pred_step) # Shape: (batch, 209, 6)\n",
        "            return res\n",
        "\n",
        "        if self.mode == 'final2':\n",
        "            x = x.reshape(-1,self.city_num, TIME_WINDOW*x.shape[-1]) # Shape: (batch, 209, 24*8)\n",
        "            x = self.x_embed(x) # Linear(24*8, x_em:32) Shape: (batch, 209, 32)\n",
        "            x = self.encoder_layer(x) # Shape: (batch, 209, 32)\n",
        "\n",
        "            \"\"\" Update \"\"\"\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            new_x, edge_w, edge_index = self.batchInput(x, edge_w, edge_index)\n",
        "            # new_x Shape: (batch, 209, 64) -> (batch*209, 64)\n",
        "            # edge_w Shape: (batch, 4112) -> (batch*4112, 1)\n",
        "            # edge_index Shape: (batch, 2, 4112) -> (2, batch*4112)\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.global_gnn[i](new_x, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "\n",
        "            \"\"\" Final Forcasting \"\"\"\n",
        "            res = self.predMLP(new_x)\n",
        "            res = res.reshape(-1,self.city_num,self.pred_step) # Shape: (batch, 209, 6)\n",
        "\n",
        "            return res\n",
        "\n",
        "\n",
        "        if self.mode == 'probe_woAttn':\n",
        "            x = x.reshape(-1,self.city_num, TIME_WINDOW*x.shape[-1]) # Shape: (batch, 209, 24*8)\n",
        "            x = self.x_embed(x) # Linear(24*8, x_em:32) Shape: (batch, 209, 32)\n",
        "\n",
        "            \"\"\" Probe \"\"\"\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            new_x, edge_w, edge_index = self.batchInput(x, edge_w, edge_index)\n",
        "            # new_x Shape: (batch, 209, 64) -> (batch*209, 64)\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.probe_gnn[i](new_x, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "            res = self.predMLP(new_x)\n",
        "            res = res.reshape(-1,self.city_num,self.pred_step) # Shape: (batch, 209, 6)\n",
        "\n",
        "            return res\n",
        "\n",
        "\n",
        "\n",
        "        ''' Self Attention '''\n",
        "        x = x.reshape(-1,x.shape[2],x.shape[3]) # Shape: (209*batch, 24, 8)\n",
        "        x = x.transpose(0,1) # Shape: (24, batch*209, 8)\n",
        "        x = self.encoder_layer(x) # self-attention, Shape: (24, batch*209, 8)\n",
        "        x = x.transpose(0,1) # Shape: (batch*209, 24, 8)\n",
        "\n",
        "\n",
        "        if self.mode == 'temp' or self.mode == 'both':\n",
        "            x2 = x.reshape(-1,self.city_num,x.shape[1],x.shape[2]) # Temporal\n",
        "            x2 = torch.index_select(x2, dim=2, index=torch.tensor([3, 7, 11, 15, 19, 23]).to(self.device)) ## ( batch size, num of city, selected 6h, 8features )\n",
        "            x2 = x2.transpose(1,2) ## ( batch size, selected 6h , num of city, 8features ) [batch size, 6, 209, 8]\n",
        "\n",
        "            h_other5 = x2[:, :-1, :, :]  # [batch size, 5, 209, 8]\n",
        "            h_other5 = h_other5.reshape(-1,h_other5.shape[1]*h_other5.shape[2],h_other5.shape[3])  # [batch size, 1045, 8]\n",
        "\n",
        "            h24 = x2[:, -1:, :, :]  # [batch size, 1, 209, 8]\n",
        "            h24 = h24.reshape(-1,h24.shape[2],h24.shape[3]) # [batch size, 209, 8]\n",
        "            h24 = h24.transpose(1,2)\n",
        "\n",
        "            attention_scores = torch.matmul(h_other5, h24)   # [batch size, 1045, 209]\n",
        "            attention_scores = attention_scores.reshape(-1,5,209,attention_scores.shape[2])  # [batch size, 5, 209, 209]\n",
        "            attention_weights = F.softmax(attention_scores, dim=1)  # [batch size, 5, 209, 209]\n",
        "\n",
        "            h_other5 = h_other5.reshape(-1,5,209,h_other5.shape[2]) # [batch size, 5, 209, 8]\n",
        "            h_other5 = h_other5.transpose(2,3) # [batch size, 5, 8, 209]\n",
        "\n",
        "            attention_weighted_sum = torch.matmul(h_other5, attention_weights) # [batch size, 5, 8, 209]\n",
        "            attention_weighted_sum = attention_weighted_sum.transpose(2,3)\n",
        "            x2 = torch.sum(attention_weighted_sum, dim=1)  # [batch size, 209, 8]\n",
        "\n",
        "        x = x.reshape(-1,self.city_num, TIME_WINDOW*x.shape[-1]) # Shape: (batch, 209, 24*8)\n",
        "        x = self.x_embed(x) # Linear(24*8, x_em:32) Shape: (batch, 209, 32)\n",
        "\n",
        "\n",
        "        if self.mode == 'probe_x':\n",
        "            \"\"\" Probe \"\"\"\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            new_x, edge_w, edge_index = self.batchInput(x, edge_w, edge_index)\n",
        "            # new_x Shape: (batch, 209, 64) -> (batch*209, 64)\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.probe_gnn[i](new_x, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "            res = self.predMLP(new_x)\n",
        "            res = res.reshape(-1,self.city_num,self.pred_step) # Shape: (batch, 209, 6)\n",
        "\n",
        "            return res\n",
        "\n",
        "\n",
        "        ''' Differentiable grouping network\n",
        "            City to City Group    '''\n",
        "\n",
        "        # S\n",
        "        w = F.softmax(self.w, dim=1) # w: (209, group_num:15)\n",
        "        w1 = w.transpose(0, 1)\n",
        "        w1 = w1.unsqueeze(dim=0)\n",
        "        w1 = w1.repeat_interleave(x.size(0), dim=0) # w1: (batch, group_num, 209)\n",
        "\n",
        "        # city group\n",
        "        loc = self.loc_embed(loc) # Linear(2, loc_em:12), shape: (batch, 209, 12)\n",
        "        x_loc = torch.cat([x,loc],dim=-1) # X, L (batch, 32+12=44)\n",
        "        g_x = torch.bmm(w1,x_loc) # g_x: (batch, group_num, 44)\n",
        "\n",
        "\n",
        "\n",
        "        ''' Group Correlation Encoding Module\n",
        "            Edge Connection '''\n",
        "\n",
        "        # T\n",
        "        u_em1 = self.u_embed1(u[:,0]) # Embedding(12, date_em=4) Shape: (batch, 209, 4)\n",
        "        u_em2 = self.u_embed2(u[:,1]) # Embedding(7, date_em=4)\n",
        "        u_em3 = self.u_embed3(u[:,2]) # Embedding(24, date_em=4)\n",
        "        u_em = torch.cat([u_em1,u_em2,u_em3],dim=-1) # Shape: (batch, 209, 12)\n",
        "\n",
        "        # Edge connection\n",
        "        for i in range(self.group_num):\n",
        "            for j in range(self.group_num):\n",
        "                if i == j: continue\n",
        "\n",
        "                # ReLU(enc(Z_i, Z_j, T))\n",
        "                g_edge_input = torch.cat([g_x[:,i],g_x[:,j],u_em],dim=-1) # Shape: (batch, 44+44+12=100)\n",
        "                tmp_g_edge_w = self.edge_inf(g_edge_input) # Shape: (batch, 12)\n",
        "\n",
        "                tmp_g_edge_w = tmp_g_edge_w.unsqueeze(dim=0) # Shape: (1, batch, 209, 12)\n",
        "                tmp_g_edge_index = torch.tensor([i,j]).unsqueeze(dim=0).to(self.device,non_blocking=True) # Shape: (1, 2)\n",
        "\n",
        "                if i == 0 and j == 1:\n",
        "                    g_edge_w = tmp_g_edge_w # Shape: (1, batch, 12)\n",
        "                    g_edge_index = tmp_g_edge_index # Shape: (1, 2)\n",
        "                else:\n",
        "                    g_edge_w = torch.cat([g_edge_w,tmp_g_edge_w],dim=0) # Shape: (210, batch, 12)\n",
        "                    g_edge_index = torch.cat([g_edge_index,tmp_g_edge_index],dim=0) # Shape: (210, 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ''' Group Message Passing\n",
        "              Group Update '''\n",
        "\n",
        "        g_edge_w = g_edge_w.transpose(0,1)\n",
        "        g_edge_index = g_edge_index.unsqueeze(dim=0)\n",
        "        g_edge_index = g_edge_index.repeat_interleave(u_em.shape[0],dim=0)\n",
        "        g_edge_index = g_edge_index.transpose(1,2)\n",
        "        g_x, g_edge_w, g_edge_index = self.batchInput(g_x, g_edge_w, g_edge_index)\n",
        "\n",
        "        for i in range(self.gnn_layer):\n",
        "            g_x = self.group_gnn[i](g_x,g_edge_index,g_edge_w)\n",
        "\n",
        "        g_x = g_x.reshape(-1,self.group_num,g_x.shape[-1])\n",
        "\n",
        "\n",
        "\n",
        "        ''' City Group to City '''\n",
        "        # S\n",
        "        w2 = w.unsqueeze(dim=0)\n",
        "        w2 = w2.repeat_interleave(g_x.size(0), dim=0)\n",
        "        new_x = torch.bmm(w2, g_x) # Shape: (batch, 209, 32)\n",
        "\n",
        "\n",
        "        if self.mode == 'both':\n",
        "            # x Shape: (batch:64, 209, 32)\n",
        "\n",
        "            \"\"\" City Update \"\"\"\n",
        "            new_x_update = torch.cat([x,new_x],dim=-1)\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            tmp_edge_index = edge_index.clone()\n",
        "            new_x_update, edge_w, tmp_edge_index = self.batchInput(new_x_update, edge_w, tmp_edge_index)\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x_update = self.global_gnn[i](new_x_update,tmp_edge_index,edge_w)\n",
        "\n",
        "            \"\"\" Temporal \"\"\"\n",
        "            x2 = x2.reshape(-1,x2.shape[-1])\n",
        "            temp_x = torch.cat([x2, new_x_update],dim=-1)\n",
        "            temp_x = self.TemporalAggregateMLP(temp_x)\n",
        "            temp_x = temp_x.reshape(-1, self.city_num, temp_x.shape[-1])\n",
        "\n",
        "            for i in range(self.pred_step):\n",
        "\n",
        "                \"\"\" Feedback \"\"\"\n",
        "                # x Shape: (batch, 209, 32)\n",
        "                new_x = torch.cat([temp_x, new_x], dim=-1) # Shape: (batch, 209, 64)\n",
        "\n",
        "\n",
        "                \"\"\" City Update \"\"\"\n",
        "                tmp_edge_index = edge_index.clone()\n",
        "                new_x, tmp_edge_w, tmp_edge_index = self.batchInput(new_x, edge_w, tmp_edge_index) # Shape: (batch*209, 64)\n",
        "\n",
        "                for j in range(self.gnn_layer):\n",
        "                    new_x = self.global_gnn[j](new_x, tmp_edge_index, tmp_edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "\n",
        "                \"\"\" Final Forcasting \"\"\"\n",
        "                # new_x = self.decoder(new_x, self.w, g_edge_index, g_edge_w, tmp_edge_index, tmp_edge_w)\n",
        "                tmp_res = self.predMLP(temp_x) # Shape: (batch*209, 1)\n",
        "                tmp_res = tmp_res.reshape(-1, self.city_num) # Shape: (batch, 209)\n",
        "                tmp_res = tmp_res.unsqueeze(dim=-1) # Shape: (batch, 209, 1)\n",
        "                if i == 0:\n",
        "                    res = tmp_res\n",
        "                else:\n",
        "                    res = torch.cat([res,tmp_res],dim=-1) # Shape: (batch, 209, i+1)\n",
        "\n",
        "                new_x = new_x.reshape(-1, self.city_num, self.x_em) # Shape: (batch, 209, 32)\n",
        "\n",
        "\n",
        "        if self.mode == 'temp':\n",
        "            \"\"\" City Update \"\"\"\n",
        "            new_x = torch.cat([x,new_x],dim=-1)\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            new_x, edge_w, edge_index = self.batchInput(new_x, edge_w, edge_index)\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.global_gnn[i](new_x,edge_index,edge_w)\n",
        "\n",
        "            \"\"\" Temporal \"\"\"\n",
        "            x2 = x2.reshape(-1,x2.shape[-1])\n",
        "            new_x = torch.cat([x2,new_x],dim=-1)\n",
        "            new_x = self.TemporalAggregateMLP(new_x)\n",
        "\n",
        "            \"\"\" Final Forcasting \"\"\"\n",
        "            new_x = self.decoder(new_x, self.w, g_edge_index, g_edge_w, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "            res = self.predMLP(new_x)\n",
        "            res = res.reshape(-1,self.city_num,self.pred_step) # Shape: (batch, 209, 6)\n",
        "\n",
        "\n",
        "        if self.mode == 'feedback':\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            output = x.reshape(-1, self.city_num, x.shape[-1])\n",
        "\n",
        "            for i in range(self.pred_step):\n",
        "\n",
        "                \"\"\" Feedback \"\"\"\n",
        "                # x Shape: (batch, 209, 32)\n",
        "                tmp_x = torch.cat([output, new_x], dim=-1) # Shape: (batch, 209, 64)\n",
        "\n",
        "\n",
        "                \"\"\" City Update \"\"\"\n",
        "                tmp_edge_index = edge_index.clone()\n",
        "                tmp_x, tmp_edge_w, tmp_edge_index = self.batchInput(tmp_x, edge_w, tmp_edge_index) # Shape: (batch*209, 64)\n",
        "                for j in range(self.gnn_layer):\n",
        "                    tmp_x = self.global_gnn[j](tmp_x, tmp_edge_index, tmp_edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "\n",
        "                \"\"\" Final Forcasting \"\"\"\n",
        "                tmp_res = self.predMLP(tmp_x) # Shape: (batch*209, 1)\n",
        "                tmp_res = tmp_res.reshape(-1, self.city_num) # Shape: (batch, 209)\n",
        "                tmp_res = tmp_res.unsqueeze(dim=-1) # Shape: (batch, 209, 1)\n",
        "                if i == 0:\n",
        "                    res = tmp_res\n",
        "                else:\n",
        "                    res = torch.cat([res,tmp_res],dim=-1) # Shape: (batch, 209, i+1)\n",
        "\n",
        "                output = tmp_x.reshape(-1, self.city_num, self.x_em) # Shape: (batch, 209, 32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if self.mode == 'probe_encoded':\n",
        "            \"\"\" City Update \"\"\"\n",
        "            new_x = torch.cat([x,new_x],dim=-1) # Shape: (batch:64, 209, 64)\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            new_x, edge_w, edge_index = self.batchInput(new_x, edge_w, edge_index)\n",
        "            # new_x Shape: (batch, 209, 64) -> (batch*209, 64)\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.global_gnn[i](new_x, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "\n",
        "            \"\"\" Probe \"\"\"\n",
        "            new_x = new_x.reshape(-1, new_x.shape[-1])\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.probe_gnn[i](new_x, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "            res = self.predMLP(new_x)\n",
        "            res = res.reshape(-1,self.city_num,self.pred_step) # Shape: (batch, 209, 6)\n",
        "\n",
        "            return res\n",
        "\n",
        "\n",
        "        if self.mode == 'feedbackDecoder':\n",
        "            \"\"\" City Update \"\"\"\n",
        "            new_x = torch.cat([x,new_x],dim=-1) # Shape: (batch:64, 209, 64)\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            new_x, edge_w, edge_index = self.batchInput(new_x, edge_w, edge_index)\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.global_gnn[i](new_x,edge_index,edge_w)\n",
        "\n",
        "\n",
        "            \"\"\" Final Forcasting \"\"\"\n",
        "            for i in range(self.pred_step):\n",
        "                new_x = self.decoder(new_x, self.w, g_edge_index, g_edge_w, edge_index, edge_w)\n",
        "                tmp_res = self.predMLP(new_x)\n",
        "                tmp_res = tmp_res.reshape(-1,self.city_num)\n",
        "                tmp_res = tmp_res.unsqueeze(dim=-1)\n",
        "                if i == 0:\n",
        "                    res = tmp_res\n",
        "                else:\n",
        "                    res = torch.cat([res,tmp_res],dim=-1)\n",
        "\n",
        "\n",
        "        if self.mode == 'baseline':\n",
        "            \"\"\" City Update \"\"\"\n",
        "            new_x = torch.cat([x,new_x],dim=-1) # Shape: (batch, 209, 64)\n",
        "            edge_w = edge_w.unsqueeze(dim=-1)\n",
        "            new_x, edge_w, edge_index = self.batchInput(new_x, edge_w, edge_index)\n",
        "            # new_x Shape: (batch, 209, 64) -> (batch*209, 64)\n",
        "            # edge_w Shape: (batch, 4112) -> (batch*4112, 1)\n",
        "            # edge_index Shape: (batch, 2, 4112) -> (2, batch*4112)\n",
        "\n",
        "            for i in range(self.gnn_layer):\n",
        "                new_x = self.global_gnn[i](new_x, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "\n",
        "\n",
        "            \"\"\" Final Forcasting \"\"\"\n",
        "            new_x = self.decoder(new_x, self.w, g_edge_index, g_edge_w, edge_index, edge_w) # Shape: (batch*209, 32)\n",
        "            res = self.predMLP(new_x)\n",
        "            res = res.reshape(-1,self.city_num,self.pred_step) # Shape: (batch, 209, 6)\n",
        "\n",
        "\n",
        "        return res\n",
        "\n",
        "class DecoderModule(nn.Module):\n",
        "    def __init__(self,x_em,edge_h,gnn_h,gnn_layer,city_num,group_num,device):\n",
        "        super(DecoderModule, self).__init__()\n",
        "        self.device = device\n",
        "        self.city_num = city_num\n",
        "        self.group_num = group_num\n",
        "        self.gnn_layer = gnn_layer\n",
        "        self.x_embed = Lin(gnn_h, x_em)\n",
        "        self.group_gnn = nn.ModuleList([NodeModel(x_em,edge_h,gnn_h)])\n",
        "        for i in range(self.gnn_layer-1):\n",
        "            self.group_gnn.append(NodeModel(gnn_h,edge_h,gnn_h))\n",
        "        self.global_gnn = nn.ModuleList([NodeModel(x_em+gnn_h,1,gnn_h)])\n",
        "        for i in range(self.gnn_layer-1):\n",
        "            self.global_gnn.append(NodeModel(gnn_h,1,gnn_h))\n",
        "\n",
        "    def forward(self, x, trans_w, g_edge_index, g_edge_w, edge_index, edge_w):\n",
        "        x = self.x_embed(x)\n",
        "        x = x.reshape(-1,self.city_num,x.shape[-1])\n",
        "\n",
        "        # S\n",
        "        w = Parameter(trans_w,requires_grad=False).to(self.device,non_blocking=True)\n",
        "        w1 = w.transpose(0,1)\n",
        "        w1 = w1.unsqueeze(dim=0)\n",
        "        w1 = w1.repeat_interleave(x.size(0), dim=0)\n",
        "        g_x = torch.bmm(w1, x)\n",
        "        g_x = g_x.reshape(-1, g_x.shape[-1])\n",
        "\n",
        "\n",
        "        for i in range(self.gnn_layer):\n",
        "            g_x = self.group_gnn[i](g_x, g_edge_index, g_edge_w)\n",
        "        g_x = g_x.reshape(-1, self.group_num, g_x.shape[-1])\n",
        "\n",
        "        # S\n",
        "        w2 = w.unsqueeze(dim=0)\n",
        "        w2 = w2.repeat_interleave(g_x.size(0), dim=0)\n",
        "        new_x = torch.bmm(w2,g_x)\n",
        "\n",
        "        # H\n",
        "        new_x = torch.cat([x,new_x],dim=-1)\n",
        "        new_x = new_x.reshape(-1,new_x.shape[-1])\n",
        "\n",
        "        for i in range(self.gnn_layer):\n",
        "            new_x = self.global_gnn[i](new_x,edge_index,edge_w)\n",
        "\n",
        "        return new_x\n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "    def __init__(self,node_h,edge_h,gnn_h):\n",
        "        super(NodeModel, self).__init__()\n",
        "        self.node_mlp_1 = Seq(Lin(node_h+edge_h,gnn_h), ReLU(inplace=False))\n",
        "        self.node_mlp_2 = Seq(Lin(node_h+gnn_h,gnn_h), ReLU(inplace=False))\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        # x: [N, F_x], where N is the number of nodes.\n",
        "        # edge_index: [2, E] with max entry N - 1.\n",
        "        # edge_attr: [E, F_e]\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[row], edge_attr], dim=1)\n",
        "        out = self.node_mlp_1(out)\n",
        "        out = scatter_mean(out, col, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1)\n",
        "        return self.node_mlp_2(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwyL2a_wOpxB"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as Data\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/AI_Project/AIP_Project-main')\n",
        "path = './data'\n",
        "\n",
        "class trainDataset(Data.Dataset):\n",
        "\tdef __init__(self, transform=None, train=True):\n",
        "\t\tself.x = np.load(os.path.join(path,'train_x.npy'),allow_pickle=True)\n",
        "\t\tself.u = np.load(os.path.join(path,'train_u.npy'),allow_pickle=True)\n",
        "\t\tself.y = np.load(os.path.join(path,'train_y.npy'),allow_pickle=True)\n",
        "\t\tself.edge_w = np.load(os.path.join(path,'edge_w.npy'),allow_pickle=True)\n",
        "\t\tself.edge_index = np.load(os.path.join(path,'edge_index.npy'),allow_pickle=True)\n",
        "\t\tself.loc = np.load(os.path.join(path,'loc_filled.npy'),allow_pickle=True)\n",
        "\t\tself.loc = self.loc.astype(float)\n",
        "\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tx = torch.FloatTensor(self.x[index]) #(24, 209, 8)\n",
        "\t\tx = x.transpose(0,1) #(209, 24, 8)\n",
        "\t\ty = torch.FloatTensor(self.y[index])\n",
        "\t\ty = y.transpose(0,1)\n",
        "\t\tu = torch.tensor(self.u[index])\n",
        "\t\tedge_index = torch.tensor(self.edge_index)\n",
        "\t\t# edge_index = edge_index.expand((x.size[0],edge_index.size[0],edge_index.size[1]))\n",
        "\t\tedge_w = torch.FloatTensor(self.edge_w)\n",
        "\t\t# edge_w = edge_w.expand((x.size[0],edge_w.size[0]))\n",
        "\t\tloc = torch.FloatTensor(self.loc)\n",
        "\n",
        "\t\treturn [x,u,y,edge_index,edge_w,loc]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.x.shape[0]\n",
        "\n",
        "class valDataset(Data.Dataset):\n",
        "\tdef __init__(self, transform=None, train=True):\n",
        "\t\tself.x = np.load(os.path.join(path,'val_x.npy'),allow_pickle=True)\n",
        "\t\tself.u = np.load(os.path.join(path,'val_u.npy'),allow_pickle=True)\n",
        "\t\tself.y = np.load(os.path.join(path,'val_y.npy'),allow_pickle=True)\n",
        "\t\tself.edge_w = np.load(os.path.join(path,'edge_w.npy'),allow_pickle=True)\n",
        "\t\tself.edge_index = np.load(os.path.join(path,'edge_index.npy'),allow_pickle=True)\n",
        "\t\tself.loc = np.load(os.path.join(path,'loc_filled.npy'),allow_pickle=True)\n",
        "\t\tself.loc = self.loc.astype(float)\n",
        "\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tx = torch.FloatTensor(self.x[index])\n",
        "\t\tx = x.transpose(0,1)\n",
        "\t\ty = torch.FloatTensor(self.y[index])\n",
        "\t\ty = y.transpose(0,1)\n",
        "\t\tu = torch.tensor(self.u[index])\n",
        "\t\tedge_index = torch.tensor(self.edge_index)\n",
        "\t\t# edge_index = edge_index.expand((x.size[0],edge_index.size[0],edge_index.size[1]))\n",
        "\t\tedge_w = torch.FloatTensor(self.edge_w)\n",
        "\t\t# edge_w = edge_w.expand((x.size[0],edge_w.size[0]))\n",
        "\t\tloc = torch.FloatTensor(self.loc)\n",
        "\n",
        "\t\treturn [x,u,y,edge_index,edge_w,loc]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.x.shape[0]\n",
        "\n",
        "class testDataset(Data.Dataset):\n",
        "\tdef __init__(self, transform=None, train=True):\n",
        "\t\tself.x = np.load(os.path.join(path,'test_x.npy'),allow_pickle=True)\n",
        "\t\tself.u = np.load(os.path.join(path,'test_u.npy'),allow_pickle=True)\n",
        "\t\tself.y = np.load(os.path.join(path,'test_y.npy'),allow_pickle=True)\n",
        "\t\tself.edge_w = np.load(os.path.join(path,'edge_w.npy'),allow_pickle=True)\n",
        "\t\tself.edge_index = np.load(os.path.join(path,'edge_index.npy'),allow_pickle=True)\n",
        "\t\tself.loc = np.load(os.path.join(path,'loc_filled.npy'),allow_pickle=True)\n",
        "\t\tself.loc = self.loc.astype(float)\n",
        "\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tx = torch.FloatTensor(self.x[index])\n",
        "\t\tx = x.transpose(0,1)\n",
        "\t\ty = torch.FloatTensor(self.y[index])\n",
        "\t\ty = y.transpose(0,1)\n",
        "\t\tu = torch.tensor(self.u[index])\n",
        "\t\tedge_index = torch.tensor(self.edge_index)\n",
        "\t\t# edge_index = edge_index.expand((x.size[0],edge_index.size[0],edge_index.size[1]))\n",
        "\t\tedge_w = torch.FloatTensor(self.edge_w)\n",
        "\t\t# edge_w = edge_w.expand((x.size[0],edge_w.size[0]))\n",
        "\t\tloc = torch.FloatTensor(self.loc)\n",
        "\n",
        "\t\treturn [x,u,y,edge_index,edge_w,loc]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.x.shape[0]"
      ],
      "metadata": {
        "id": "Tb1LwWq48O6k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --run_time 1 --mode both"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h05mUurmjbXB",
        "outputId": "11e45fa8-83f5-4cc8-ee38-103920149732"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(device='cuda', mode='both', encoder='self', w_init='rand', mark='', run_times=1, epoch=300, batch_size=64, w_rate=50, city_num=209, group_num=15, gnn_h=32, gnn_layer=2, x_em=32, date_em=4, loc_em=12, edge_h=12, lr=0.001, wd=0.001, pred_step=6, data_usage=1.0, test='')\n",
            "Dataload Finished\n",
            "Model Created\n",
            "city_model: Trainable, 33460\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [1/300], Train_Loss: 51707.4726, Val_Loss: 17983.6338\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [2/300], Train_Loss: 22715.1814, Val_Loss: 14651.6212\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [3/300], Train_Loss: 19008.1942, Val_Loss: 12843.2536\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [4/300], Train_Loss: 17785.9091, Val_Loss: 12868.5904\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [5/300], Train_Loss: 17127.0416, Val_Loss: 11804.5980\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [6/300], Train_Loss: 16740.5727, Val_Loss: 11939.5427\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [7/300], Train_Loss: 16505.4271, Val_Loss: 11755.9423\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [8/300], Train_Loss: 16285.5101, Val_Loss: 11548.3391\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [9/300], Train_Loss: 16146.4179, Val_Loss: 11319.0426\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [10/300], Train_Loss: 16011.3342, Val_Loss: 11512.7224\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [11/300], Train_Loss: 15920.7046, Val_Loss: 11229.3269\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [12/300], Train_Loss: 15845.1920, Val_Loss: 11095.7106\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [13/300], Train_Loss: 15735.6157, Val_Loss: 11113.7085\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [14/300], Train_Loss: 15648.7961, Val_Loss: 11459.2458\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [15/300], Train_Loss: 15588.3381, Val_Loss: 10936.3263\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [16/300], Train_Loss: 15559.8122, Val_Loss: 10929.9646\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [17/300], Train_Loss: 15437.3127, Val_Loss: 10912.0255\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [18/300], Train_Loss: 15401.5242, Val_Loss: 11086.9148\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [19/300], Train_Loss: 15358.7102, Val_Loss: 10908.4896\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [20/300], Train_Loss: 15331.8786, Val_Loss: 10807.5696\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [21/300], Train_Loss: 15271.6029, Val_Loss: 11060.8857\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [22/300], Train_Loss: 15256.2078, Val_Loss: 11569.9380\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [23/300], Train_Loss: 15213.6188, Val_Loss: 11278.1677\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [24/300], Train_Loss: 15136.0112, Val_Loss: 11278.8705\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [25/300], Train_Loss: 15134.5472, Val_Loss: 10675.5042\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [26/300], Train_Loss: 15085.6759, Val_Loss: 10693.5297\n",
            "100% 223/223 [00:52<00:00,  4.22it/s]\n",
            "Epoch [27/300], Train_Loss: 15055.0887, Val_Loss: 10642.3226\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [28/300], Train_Loss: 15005.9425, Val_Loss: 10683.4212\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [29/300], Train_Loss: 15002.9497, Val_Loss: 10772.0712\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [30/300], Train_Loss: 14957.0221, Val_Loss: 10599.9653\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [31/300], Train_Loss: 14911.6012, Val_Loss: 10604.1396\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [32/300], Train_Loss: 14912.7751, Val_Loss: 10565.8133\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [33/300], Train_Loss: 14907.7117, Val_Loss: 10558.0460\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [34/300], Train_Loss: 14884.4453, Val_Loss: 10673.5916\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [35/300], Train_Loss: 14832.1973, Val_Loss: 10527.6743\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [36/300], Train_Loss: 14802.1151, Val_Loss: 10560.2426\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [37/300], Train_Loss: 14810.8702, Val_Loss: 10537.1361\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [38/300], Train_Loss: 14773.1342, Val_Loss: 10481.7429\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [39/300], Train_Loss: 14782.7984, Val_Loss: 10592.1680\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [40/300], Train_Loss: 14797.8405, Val_Loss: 10478.7393\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [41/300], Train_Loss: 14723.2715, Val_Loss: 10461.4594\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [42/300], Train_Loss: 14722.2361, Val_Loss: 10572.2233\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [43/300], Train_Loss: 14737.2469, Val_Loss: 10767.9031\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [44/300], Train_Loss: 14700.6297, Val_Loss: 10829.6878\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [45/300], Train_Loss: 14658.4117, Val_Loss: 10405.9955\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [46/300], Train_Loss: 14667.8388, Val_Loss: 10423.7451\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [47/300], Train_Loss: 14644.5373, Val_Loss: 10496.3859\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [48/300], Train_Loss: 14635.7472, Val_Loss: 10427.8713\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [49/300], Train_Loss: 14637.1174, Val_Loss: 10487.9004\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [50/300], Train_Loss: 14608.3609, Val_Loss: 10517.2874\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [51/300], Train_Loss: 14580.7089, Val_Loss: 10546.2528\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [52/300], Train_Loss: 14583.2276, Val_Loss: 10392.7928\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [53/300], Train_Loss: 14599.0341, Val_Loss: 10383.4847\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [54/300], Train_Loss: 14542.3318, Val_Loss: 10506.0836\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [55/300], Train_Loss: 14549.4401, Val_Loss: 10515.8741\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [56/300], Train_Loss: 14528.4085, Val_Loss: 10512.2798\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [57/300], Train_Loss: 14553.0076, Val_Loss: 10414.2963\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [58/300], Train_Loss: 14510.7858, Val_Loss: 10563.3818\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [59/300], Train_Loss: 14511.8907, Val_Loss: 10371.2730\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [60/300], Train_Loss: 14554.9030, Val_Loss: 10479.9776\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [61/300], Train_Loss: 14492.8947, Val_Loss: 10364.1694\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [62/300], Train_Loss: 14498.1266, Val_Loss: 10701.0634\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [63/300], Train_Loss: 14485.3657, Val_Loss: 10329.7551\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [64/300], Train_Loss: 14461.8462, Val_Loss: 10378.3542\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [65/300], Train_Loss: 14456.8275, Val_Loss: 10361.9086\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [66/300], Train_Loss: 14441.2887, Val_Loss: 10690.0541\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [67/300], Train_Loss: 14460.2825, Val_Loss: 10515.6562\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [68/300], Train_Loss: 14447.6188, Val_Loss: 10355.6449\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [69/300], Train_Loss: 14423.6504, Val_Loss: 10376.4880\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [70/300], Train_Loss: 14416.3110, Val_Loss: 10302.9184\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [71/300], Train_Loss: 14414.1591, Val_Loss: 10344.9051\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [72/300], Train_Loss: 14425.8320, Val_Loss: 10378.1760\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [73/300], Train_Loss: 14421.6548, Val_Loss: 10560.2813\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [74/300], Train_Loss: 14369.9599, Val_Loss: 10285.8637\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [75/300], Train_Loss: 14376.5741, Val_Loss: 10416.3605\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [76/300], Train_Loss: 14375.9310, Val_Loss: 10295.2577\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [77/300], Train_Loss: 14391.2176, Val_Loss: 10284.3045\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [78/300], Train_Loss: 14360.3804, Val_Loss: 10268.6535\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [79/300], Train_Loss: 14353.0339, Val_Loss: 10260.9829\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [80/300], Train_Loss: 14353.5564, Val_Loss: 10277.0085\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [81/300], Train_Loss: 14355.2557, Val_Loss: 10357.6816\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [82/300], Train_Loss: 14338.6884, Val_Loss: 10253.4120\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [83/300], Train_Loss: 14330.7372, Val_Loss: 10417.8525\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [84/300], Train_Loss: 14311.4136, Val_Loss: 10314.4057\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [85/300], Train_Loss: 14303.2581, Val_Loss: 10281.4310\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [86/300], Train_Loss: 14291.3404, Val_Loss: 10254.7406\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [87/300], Train_Loss: 14304.4188, Val_Loss: 10424.4589\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [88/300], Train_Loss: 14285.9346, Val_Loss: 10255.6901\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [89/300], Train_Loss: 14280.7126, Val_Loss: 10287.8393\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [90/300], Train_Loss: 14276.0451, Val_Loss: 10248.7406\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [91/300], Train_Loss: 14276.4229, Val_Loss: 10461.0137\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [92/300], Train_Loss: 14247.4856, Val_Loss: 10251.6560\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [93/300], Train_Loss: 14252.1265, Val_Loss: 10323.6984\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [94/300], Train_Loss: 14245.4050, Val_Loss: 10307.2248\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [95/300], Train_Loss: 14233.2204, Val_Loss: 10224.4830\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [96/300], Train_Loss: 14233.9048, Val_Loss: 10324.7015\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [97/300], Train_Loss: 14219.9008, Val_Loss: 10271.8084\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [98/300], Train_Loss: 14235.4126, Val_Loss: 10237.2683\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [99/300], Train_Loss: 14206.0587, Val_Loss: 10192.4976\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [100/300], Train_Loss: 14210.1639, Val_Loss: 10391.6872\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [101/300], Train_Loss: 14205.4557, Val_Loss: 10297.8801\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [102/300], Train_Loss: 14215.5229, Val_Loss: 10302.8197\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [103/300], Train_Loss: 14190.9659, Val_Loss: 10654.7461\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [104/300], Train_Loss: 14182.0327, Val_Loss: 10197.7761\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [105/300], Train_Loss: 14164.5269, Val_Loss: 10222.7189\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [106/300], Train_Loss: 14157.8199, Val_Loss: 10181.5999\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [107/300], Train_Loss: 14143.3647, Val_Loss: 10194.6284\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [108/300], Train_Loss: 14157.2728, Val_Loss: 10190.9283\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [109/300], Train_Loss: 14133.4516, Val_Loss: 10213.4639\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [110/300], Train_Loss: 14173.5364, Val_Loss: 10233.0429\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [111/300], Train_Loss: 14132.4317, Val_Loss: 10149.7898\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [112/300], Train_Loss: 14137.8785, Val_Loss: 10241.2251\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [113/300], Train_Loss: 14120.1718, Val_Loss: 10175.7528\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [114/300], Train_Loss: 14124.1259, Val_Loss: 10459.1113\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [115/300], Train_Loss: 14118.8866, Val_Loss: 10178.2713\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [116/300], Train_Loss: 14110.4201, Val_Loss: 10176.9643\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [117/300], Train_Loss: 14097.7404, Val_Loss: 10151.9229\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [118/300], Train_Loss: 14085.1949, Val_Loss: 10156.0120\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [119/300], Train_Loss: 14092.6947, Val_Loss: 10185.5371\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [120/300], Train_Loss: 14087.0248, Val_Loss: 10132.3218\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [121/300], Train_Loss: 14068.4976, Val_Loss: 10104.8594\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [122/300], Train_Loss: 14062.4325, Val_Loss: 10294.7654\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [123/300], Train_Loss: 14068.4332, Val_Loss: 10243.4545\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [124/300], Train_Loss: 14079.0704, Val_Loss: 10149.9122\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [125/300], Train_Loss: 14063.2402, Val_Loss: 10102.8724\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [126/300], Train_Loss: 14047.1592, Val_Loss: 10101.3471\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [127/300], Train_Loss: 14057.2118, Val_Loss: 10170.6606\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [128/300], Train_Loss: 14030.1207, Val_Loss: 10105.3176\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [129/300], Train_Loss: 14038.2365, Val_Loss: 10139.5777\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [130/300], Train_Loss: 14025.0566, Val_Loss: 10118.6361\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [131/300], Train_Loss: 14020.5009, Val_Loss: 10289.7209\n",
            "100% 223/223 [00:52<00:00,  4.24it/s]\n",
            "Epoch [132/300], Train_Loss: 14033.2989, Val_Loss: 10099.0638\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [133/300], Train_Loss: 14004.9829, Val_Loss: 10082.1035\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [134/300], Train_Loss: 14009.4827, Val_Loss: 10124.3136\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [135/300], Train_Loss: 14000.9712, Val_Loss: 10171.8721\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [136/300], Train_Loss: 14006.2801, Val_Loss: 10084.0062\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [137/300], Train_Loss: 13994.5540, Val_Loss: 10091.1888\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [138/300], Train_Loss: 13991.3685, Val_Loss: 10098.5483\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [139/300], Train_Loss: 13991.6966, Val_Loss: 10203.5174\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [140/300], Train_Loss: 13976.6982, Val_Loss: 10188.1562\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [141/300], Train_Loss: 13967.1932, Val_Loss: 10107.7352\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [142/300], Train_Loss: 13977.4779, Val_Loss: 10104.5372\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [143/300], Train_Loss: 13984.9550, Val_Loss: 10079.7596\n",
            "100% 223/223 [00:53<00:00,  4.14it/s]\n",
            "Epoch [144/300], Train_Loss: 13973.3950, Val_Loss: 10283.9354\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [145/300], Train_Loss: 13952.5475, Val_Loss: 10088.5917\n",
            "100% 223/223 [00:52<00:00,  4.23it/s]\n",
            "Epoch [146/300], Train_Loss: 13944.5728, Val_Loss: 10087.7196\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [147/300], Train_Loss: 13951.1311, Val_Loss: 10072.5409\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [148/300], Train_Loss: 13937.9452, Val_Loss: 10077.3661\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [149/300], Train_Loss: 13947.9704, Val_Loss: 10102.3660\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [150/300], Train_Loss: 13949.5345, Val_Loss: 10185.4975\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [151/300], Train_Loss: 13950.9906, Val_Loss: 10069.1872\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [152/300], Train_Loss: 13926.3025, Val_Loss: 10151.7435\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [153/300], Train_Loss: 13923.2781, Val_Loss: 10053.3626\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [154/300], Train_Loss: 13924.2796, Val_Loss: 10065.3264\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [155/300], Train_Loss: 13924.6088, Val_Loss: 10296.0555\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [156/300], Train_Loss: 13909.6139, Val_Loss: 10029.4807\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [157/300], Train_Loss: 13925.8847, Val_Loss: 10140.8214\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [158/300], Train_Loss: 13897.5622, Val_Loss: 10099.3777\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [159/300], Train_Loss: 13911.4465, Val_Loss: 10034.7977\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [160/300], Train_Loss: 13903.3550, Val_Loss: 10080.9983\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [161/300], Train_Loss: 13901.1662, Val_Loss: 10024.5304\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [162/300], Train_Loss: 13916.3606, Val_Loss: 10094.5802\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [163/300], Train_Loss: 13874.8593, Val_Loss: 10031.0737\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [164/300], Train_Loss: 13870.8485, Val_Loss: 10058.4392\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [165/300], Train_Loss: 13880.7790, Val_Loss: 10017.9092\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [166/300], Train_Loss: 13893.1886, Val_Loss: 10052.0933\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [167/300], Train_Loss: 13870.9718, Val_Loss: 10016.7359\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [168/300], Train_Loss: 13870.6217, Val_Loss: 10019.2178\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [169/300], Train_Loss: 13870.5534, Val_Loss: 10064.7929\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [170/300], Train_Loss: 13867.8793, Val_Loss: 10035.4452\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [171/300], Train_Loss: 13874.6025, Val_Loss: 10025.2362\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [172/300], Train_Loss: 13860.3723, Val_Loss: 10052.6884\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [173/300], Train_Loss: 13857.1725, Val_Loss: 10134.0769\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [174/300], Train_Loss: 13851.6705, Val_Loss: 10178.0687\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [175/300], Train_Loss: 13842.1091, Val_Loss: 10036.6862\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [176/300], Train_Loss: 13833.5237, Val_Loss: 10104.4570\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [177/300], Train_Loss: 13859.6193, Val_Loss: 10053.3772\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [178/300], Train_Loss: 13847.5822, Val_Loss: 10039.7213\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [179/300], Train_Loss: 13843.1759, Val_Loss: 10003.6227\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [180/300], Train_Loss: 13838.5528, Val_Loss: 10002.8343\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [181/300], Train_Loss: 13842.4561, Val_Loss: 9987.8085\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [182/300], Train_Loss: 13830.8988, Val_Loss: 10079.5371\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [183/300], Train_Loss: 13834.0260, Val_Loss: 10007.7833\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [184/300], Train_Loss: 13831.2782, Val_Loss: 10038.7663\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [185/300], Train_Loss: 13833.9892, Val_Loss: 10124.6398\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [186/300], Train_Loss: 13816.3664, Val_Loss: 10021.4294\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [187/300], Train_Loss: 13813.4964, Val_Loss: 9981.0270\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [188/300], Train_Loss: 13801.1933, Val_Loss: 10033.8572\n",
            "100% 223/223 [00:52<00:00,  4.25it/s]\n",
            "Epoch [189/300], Train_Loss: 13821.0245, Val_Loss: 9996.9965\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [190/300], Train_Loss: 13808.8372, Val_Loss: 10057.5360\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [191/300], Train_Loss: 13816.5127, Val_Loss: 9998.8781\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [192/300], Train_Loss: 13808.9813, Val_Loss: 10090.7027\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [193/300], Train_Loss: 13792.4802, Val_Loss: 9974.2238\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [194/300], Train_Loss: 13788.8025, Val_Loss: 9965.3400\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [195/300], Train_Loss: 13792.9928, Val_Loss: 9984.0737\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [196/300], Train_Loss: 13794.5310, Val_Loss: 10035.5851\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [197/300], Train_Loss: 13782.9016, Val_Loss: 10001.8065\n",
            "100% 223/223 [00:52<00:00,  4.26it/s]\n",
            "Epoch [198/300], Train_Loss: 13797.9950, Val_Loss: 10047.6975\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [199/300], Train_Loss: 13795.7849, Val_Loss: 10054.0737\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [200/300], Train_Loss: 13796.2516, Val_Loss: 10047.1937\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [201/300], Train_Loss: 13777.9767, Val_Loss: 10023.6588\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [202/300], Train_Loss: 13792.8472, Val_Loss: 9973.7730\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [203/300], Train_Loss: 13766.6792, Val_Loss: 10017.6775\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [204/300], Train_Loss: 13776.9970, Val_Loss: 10062.4529\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [205/300], Train_Loss: 13768.2168, Val_Loss: 10003.4453\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [206/300], Train_Loss: 13754.2601, Val_Loss: 10001.6082\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [207/300], Train_Loss: 13769.4517, Val_Loss: 9992.6042\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [208/300], Train_Loss: 13784.2608, Val_Loss: 10094.1881\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [209/300], Train_Loss: 13769.5273, Val_Loss: 9992.6909\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [210/300], Train_Loss: 13771.3383, Val_Loss: 10083.2940\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [211/300], Train_Loss: 13761.0432, Val_Loss: 10046.3600\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [212/300], Train_Loss: 13761.7014, Val_Loss: 10022.1388\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [213/300], Train_Loss: 13770.2695, Val_Loss: 9952.7377\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [214/300], Train_Loss: 13743.6247, Val_Loss: 9960.7443\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [215/300], Train_Loss: 13751.9778, Val_Loss: 10039.3974\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [216/300], Train_Loss: 13748.8312, Val_Loss: 9960.4984\n",
            "100% 223/223 [00:52<00:00,  4.27it/s]\n",
            "Epoch [217/300], Train_Loss: 13734.8815, Val_Loss: 9980.2395\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [218/300], Train_Loss: 13760.4341, Val_Loss: 9991.5676\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [219/300], Train_Loss: 13739.7423, Val_Loss: 10031.3625\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [220/300], Train_Loss: 13739.9172, Val_Loss: 9969.7063\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [221/300], Train_Loss: 13735.6104, Val_Loss: 10046.3657\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [222/300], Train_Loss: 13751.2521, Val_Loss: 9994.3434\n",
            "100% 223/223 [00:51<00:00,  4.35it/s]\n",
            "Epoch [223/300], Train_Loss: 13742.7246, Val_Loss: 10009.7172\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [224/300], Train_Loss: 13733.3033, Val_Loss: 10152.4393\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [225/300], Train_Loss: 13726.6211, Val_Loss: 10007.6273\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [226/300], Train_Loss: 13726.1505, Val_Loss: 9994.7100\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [227/300], Train_Loss: 13731.0180, Val_Loss: 9937.1363\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [228/300], Train_Loss: 13734.4363, Val_Loss: 9950.6315\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [229/300], Train_Loss: 13719.8395, Val_Loss: 9975.8330\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [230/300], Train_Loss: 13710.9483, Val_Loss: 9948.9165\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [231/300], Train_Loss: 13713.8235, Val_Loss: 9955.8512\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [232/300], Train_Loss: 13710.9237, Val_Loss: 9951.2168\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [233/300], Train_Loss: 13734.2417, Val_Loss: 9955.3027\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [234/300], Train_Loss: 13709.0636, Val_Loss: 9977.4787\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [235/300], Train_Loss: 13707.7818, Val_Loss: 9971.5167\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [236/300], Train_Loss: 13722.1117, Val_Loss: 9964.3293\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [237/300], Train_Loss: 13707.0871, Val_Loss: 9934.0718\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [238/300], Train_Loss: 13709.8567, Val_Loss: 9973.1600\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [239/300], Train_Loss: 13731.4333, Val_Loss: 9979.4954\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [240/300], Train_Loss: 13701.6379, Val_Loss: 10028.3940\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [241/300], Train_Loss: 13701.2272, Val_Loss: 9942.0207\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [242/300], Train_Loss: 13693.7754, Val_Loss: 9953.1353\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [243/300], Train_Loss: 13697.6318, Val_Loss: 10004.4132\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [244/300], Train_Loss: 13686.9085, Val_Loss: 9957.2841\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [245/300], Train_Loss: 13725.6211, Val_Loss: 9976.2237\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [246/300], Train_Loss: 13692.6876, Val_Loss: 9941.3784\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [247/300], Train_Loss: 13703.9137, Val_Loss: 9995.3028\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [248/300], Train_Loss: 13687.9848, Val_Loss: 9970.6791\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [249/300], Train_Loss: 13689.7075, Val_Loss: 9935.5610\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [250/300], Train_Loss: 13688.9601, Val_Loss: 9944.5244\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [251/300], Train_Loss: 13682.6812, Val_Loss: 9970.1192\n",
            "100% 223/223 [00:51<00:00,  4.35it/s]\n",
            "Epoch [252/300], Train_Loss: 13674.2286, Val_Loss: 9956.1566\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [253/300], Train_Loss: 13688.8970, Val_Loss: 10034.6507\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [254/300], Train_Loss: 13683.6550, Val_Loss: 9953.4883\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [255/300], Train_Loss: 13676.1199, Val_Loss: 9984.4163\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [256/300], Train_Loss: 13667.7276, Val_Loss: 9969.1181\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [257/300], Train_Loss: 13674.3445, Val_Loss: 9969.8879\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [258/300], Train_Loss: 13676.2264, Val_Loss: 10149.2396\n",
            "100% 223/223 [00:51<00:00,  4.35it/s]\n",
            "Epoch [259/300], Train_Loss: 13672.4618, Val_Loss: 9948.1629\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [260/300], Train_Loss: 13654.6256, Val_Loss: 9998.3396\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [261/300], Train_Loss: 13674.5972, Val_Loss: 10095.2416\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [262/300], Train_Loss: 13666.0690, Val_Loss: 9961.4341\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [263/300], Train_Loss: 13667.9359, Val_Loss: 9940.8299\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [264/300], Train_Loss: 13655.9967, Val_Loss: 9916.6857\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [265/300], Train_Loss: 13679.9288, Val_Loss: 9941.2530\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [266/300], Train_Loss: 13654.2584, Val_Loss: 9942.2550\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [267/300], Train_Loss: 13659.6061, Val_Loss: 9971.7920\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [268/300], Train_Loss: 13660.1840, Val_Loss: 9954.4631\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [269/300], Train_Loss: 13653.1077, Val_Loss: 9942.2116\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [270/300], Train_Loss: 13659.9875, Val_Loss: 10012.7564\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [271/300], Train_Loss: 13658.4818, Val_Loss: 10193.4793\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [272/300], Train_Loss: 13647.2175, Val_Loss: 9970.4886\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [273/300], Train_Loss: 13647.2048, Val_Loss: 9950.9268\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [274/300], Train_Loss: 13665.7408, Val_Loss: 9940.1781\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [275/300], Train_Loss: 13641.2545, Val_Loss: 10023.9878\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [276/300], Train_Loss: 13639.8870, Val_Loss: 9950.6554\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [277/300], Train_Loss: 13634.1916, Val_Loss: 9968.5262\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [278/300], Train_Loss: 13650.6891, Val_Loss: 9930.7489\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [279/300], Train_Loss: 13634.6394, Val_Loss: 9941.0557\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [280/300], Train_Loss: 13643.8189, Val_Loss: 9923.8531\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [281/300], Train_Loss: 13632.4641, Val_Loss: 9959.2541\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [282/300], Train_Loss: 13640.8240, Val_Loss: 9987.2631\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [283/300], Train_Loss: 13647.8454, Val_Loss: 9958.6061\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [284/300], Train_Loss: 13630.4426, Val_Loss: 10018.0339\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [285/300], Train_Loss: 13636.3332, Val_Loss: 9982.6954\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [286/300], Train_Loss: 13628.0626, Val_Loss: 9972.1095\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [287/300], Train_Loss: 13630.3320, Val_Loss: 9918.1836\n",
            "100% 223/223 [00:51<00:00,  4.33it/s]\n",
            "Epoch [288/300], Train_Loss: 13638.6720, Val_Loss: 9946.8070\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [289/300], Train_Loss: 13626.9617, Val_Loss: 10034.6824\n",
            "100% 223/223 [00:51<00:00,  4.31it/s]\n",
            "Epoch [290/300], Train_Loss: 13615.0970, Val_Loss: 10044.6877\n",
            "100% 223/223 [00:51<00:00,  4.34it/s]\n",
            "Epoch [291/300], Train_Loss: 13631.6054, Val_Loss: 10018.6242\n",
            "100% 223/223 [00:51<00:00,  4.32it/s]\n",
            "Epoch [292/300], Train_Loss: 13616.5779, Val_Loss: 10109.9340\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [293/300], Train_Loss: 13619.1893, Val_Loss: 10012.9776\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [294/300], Train_Loss: 13620.9535, Val_Loss: 10013.7255\n",
            "100% 223/223 [00:51<00:00,  4.30it/s]\n",
            "Epoch [295/300], Train_Loss: 13622.4571, Val_Loss: 9955.1259\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [296/300], Train_Loss: 13633.9044, Val_Loss: 9996.9772\n",
            "100% 223/223 [00:51<00:00,  4.29it/s]\n",
            "Epoch [297/300], Train_Loss: 13615.9426, Val_Loss: 9927.1489\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [298/300], Train_Loss: 13608.5377, Val_Loss: 9955.0222\n",
            "100% 223/223 [00:52<00:00,  4.29it/s]\n",
            "Epoch [299/300], Train_Loss: 13606.7830, Val_Loss: 9914.3054\n",
            "100% 223/223 [00:52<00:00,  4.28it/s]\n",
            "Epoch [300/300], Train_Loss: 13607.1215, Val_Loss: 10115.2077\n",
            "Running time: 16808.95572590828 Seconds\n",
            "mae: [ 7.851461  9.198957 10.847953 12.609352 14.34903  15.977933]\n",
            "rmse: [14.94755  17.292795 20.023964 22.718475 25.282494 27.606289]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Argument"
      ],
      "metadata": {
        "id": "QHmto_V4_IGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import shap\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# 설정된 변수\n",
        "device = 'cuda'\n",
        "mode = 'both'\n",
        "encoder = 'self'\n",
        "w_init = 'rand'\n",
        "mark = ''\n",
        "batch_size = 4\n",
        "city_num = 209\n",
        "group_num = 15\n",
        "gnn_h = 32\n",
        "gnn_layer = 2\n",
        "x_em = 32\n",
        "date_em = 4\n",
        "loc_em = 12\n",
        "edge_h = 12\n",
        "pred_step = 6\n",
        "data_usage = 1.0\n",
        "path = '/content/drive/MyDrive/AI_Project/AIP_Project-main/data'"
      ],
      "metadata": {
        "id": "Q0nW9KJ_NpDc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Model and Data"
      ],
      "metadata": {
        "id": "eObdDDLR8aze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드\n",
        "train_dataset = trainDataset()\n",
        "val_dataset = valDataset()\n",
        "test_dataset = testDataset()\n",
        "\n",
        "train_p = list(range(0, int(len(train_dataset) * data_usage)))\n",
        "val_p = list(range(0, int(len(val_dataset) * data_usage)))\n",
        "test_p = list(range(0, int(len(test_dataset) * data_usage)))\n",
        "\n",
        "train_dataset = Subset(train_dataset, train_p)\n",
        "val_dataset = Subset(val_dataset, val_p)\n",
        "test_dataset = Subset(test_dataset, test_p)"
      ],
      "metadata": {
        "id": "5jdjKBTLNv5Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "device = device\n",
        "\n",
        "print(\"Dataload Finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do6fNeOVBE1j",
        "outputId": "74fef1f8-4d21-413b-a182-b7d6bf29a6d8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataload Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 모델 준비\n",
        "w = None\n",
        "if w_init == 'group':\n",
        "    city_loc = np.load(os.path.join(path, 'loc_filled.npy'), allow_pickle=True)\n",
        "    kmeans = KMeans(n_clusters=group_num, random_state=0).fit(city_loc)\n",
        "    group_list = kmeans.labels_.tolist()\n",
        "    w = np.random.randn(city_num, group_num)\n",
        "    w = w * 0.1\n",
        "    for i in range(len(group_list)):\n",
        "        w[i, group_list[i]] = 1.0\n",
        "    w = torch.FloatTensor(w).to(device, non_blocking=True)\n",
        "\n",
        "city_model = Model(mode, encoder, w_init, w, x_em, date_em, loc_em, edge_h, gnn_h, gnn_layer, city_num, group_num, pred_step, device).to(device)\n",
        "print(\"Model Created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX-itfkDG9QK",
        "outputId": "4e90652f-7660-4f43-b206-7b5eff46821d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP"
      ],
      "metadata": {
        "id": "mcRu-iU9G_Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트에서 불일치하는 레이어 제거\n",
        "checkpoint = torch.load('self_para_.ckpt')\n",
        "checkpoint.pop('predMLP.2.weight', None)\n",
        "checkpoint.pop('predMLP.2.bias', None)\n",
        "\n",
        "# 모델에 체크포인트 로드\n",
        "city_model.load_state_dict(checkpoint, strict=False)\n",
        "city_model.eval()\n",
        "\n",
        "# SHAP 분석 준비\n",
        "data_iter = iter(test_loader)\n",
        "sample_data = next(data_iter)  # 첫 번째 배치 사용\n",
        "sample_data = [item.to(device, non_blocking=True) for item in sample_data]\n",
        "x_sample, u_sample, y_sample, edge_index_sample, edge_w_sample, loc_sample = sample_data"
      ],
      "metadata": {
        "id": "7RQEDh0ewD2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 모델 로드\n",
        "checkpoint = torch.load('self_para_.ckpt')\n",
        "city_model.load_state_dict(checkpoint, strict=False)\n",
        "city_model.eval()\n",
        "\n",
        "# SHAP 분석 준비\n",
        "data_iter = iter(test_loader)\n",
        "sample_data = next(data_iter)  # 첫 번째 배치 사용\n",
        "sample_data = [item.to(device, non_blocking=True) for item in sample_data]\n",
        "x_sample, u_sample, y_sample, edge_index_sample, edge_w_sample, loc_sample = sample_data"
      ],
      "metadata": {
        "id": "hoIyeTohFa9q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# 모델 입력 데이터를 float 타입으로 변환\n",
        "inputs = [x_sample, u_sample.float(), edge_index_sample.float(), edge_w_sample, loc_sample]\n",
        "\n",
        "# 모델 내부에서 필요한 부분만 long 타입으로 변환\n",
        "class WrappedModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(WrappedModel, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x_sample, u_sample, edge_index_sample, edge_w_sample, loc_sample):\n",
        "        return self.model(x_sample, u_sample.long(), edge_index_sample.long(), edge_w_sample, loc_sample)\n",
        "\n",
        "wrapped_model = WrappedModel(city_model)\n",
        "\n",
        "# SHAP explainer 생성\n",
        "explainer = shap.DeepExplainer(wrapped_model, inputs)\n",
        "\n",
        "# 입력 데이터를 작은 배치로 나누기\n",
        "batch_size = 1  # 필요에 따라 조정\n",
        "num_batches = x_sample.shape[0] // batch_size\n",
        "shap_values = []\n",
        "\n",
        "for i in range(num_batches):\n",
        "    print(f\"Processing batch {i + 1}/{num_batches}\")\n",
        "    batch_inputs = [x_sample[i * batch_size:(i + 1) * batch_size],\n",
        "                    u_sample[i * batch_size:(i + 1) * batch_size].float(),\n",
        "                    edge_index_sample[i * batch_size:(i + 1) * batch_size].float(),\n",
        "                    edge_w_sample[i * batch_size:(i + 1) * batch_size],\n",
        "                    loc_sample[i * batch_size:(i + 1) * batch_size]]\n",
        "\n",
        "    batch_shap_values = explainer.shap_values(batch_inputs, check_additivity=False)\n",
        "    shap_values.append(batch_shap_values)\n",
        "\n",
        "    # 메모리 정리\n",
        "    del batch_inputs, batch_shap_values\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "9zjXUBc19jAZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "5c8d0f1f-fb57-4549-cc7f-2dcd7e4f8ac6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1/4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "grad can be implicitly created only for scalar outputs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7a2de6c078ff>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                     loc_sample[i * batch_size:(i + 1) * batch_size]]\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mbatch_shap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mshap_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shap_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \"\"\"\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_deep/deep_pytorch.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# run attribution computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0msample_phis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_deep/deep_pytorch.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 grad = torch.autograd.grad(selected, x,\n\u001b[0m\u001b[1;32m    120\u001b[0m                                            \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                                            allow_unused=True)[0]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0mgrad_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     grad_outputs_ = _make_grads(\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_grads_batched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    134\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP 값 합치기\n",
        "shap_values = [np.concatenate([batch[i] for batch in shap_values], axis=0) for i in range(len(shap_values[0]))]"
      ],
      "metadata": {
        "id": "yMyqOMs_ef69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "41814256-2d39-40dd-e55b-88c24fd2d3e6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-dfb7069f1f85>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SHAP 값 합치기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshap_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, shap_value in enumerate(shap_values):\n",
        "    print(f\"shap_value[{i}].shape: {np.array(shap_value).shape}\")\n",
        "\n",
        "print(shap_values[0][0][0][1][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKBWsR_-fZon",
        "outputId": "73346fbc-f81b-4789-a5f3-02d87d27d8b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shap_value[0].shape: (4, 209, 24, 8, 209)\n",
            "shap_value[1].shape: (4, 3, 209)\n",
            "shap_value[2].shape: (4, 2, 4112, 209)\n",
            "shap_value[3].shape: (4, 4112, 209)\n",
            "shap_value[4].shape: (4, 209, 2, 209)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = 0  # 시각화할 샘플 인덱스\n",
        "\n",
        "# shap_values는 이미 계산된 SHAP 값입니다.\n",
        "shap_values_np = [np.array(shap_value) for shap_value in shap_values]\n",
        "\n",
        "# 시각화할 샘플의 SHAP 값을 선택\n",
        "shap_values_sample = [shap_value[sample_index] for shap_value in shap_values_np]\n",
        "\n",
        "# base_values와 data도 샘플에 맞게 선택\n",
        "base_values_sample = explainer.expected_value[0]  # 첫 번째 클래스의 base value, 필요에 따라 조정\n",
        "data_sample = [x_sample[sample_index].cpu().numpy(),\n",
        "               u_sample[sample_index].cpu().numpy(),\n",
        "               edge_index_sample[sample_index].cpu().numpy(),\n",
        "               edge_w_sample[sample_index].cpu().numpy(),\n",
        "               loc_sample[sample_index].cpu().numpy()]\n",
        "\n",
        "# SHAP 값이 여러 차원을 가지고 있으므로, 하나의 차원을 선택하여 시각화\n",
        "# 예시로 마지막 차원을 선택하여 1차원 벡터로 축소\n",
        "for i, shap_value in enumerate(shap_values_sample):\n",
        "    if len(shap_value.shape) > 1:\n",
        "        # 여러 차원을 가진 경우, 마지막 차원을 선택 (예시: [:, :] 선택)\n",
        "        shap_value = shap_value[-1, :]  # 마지막 차원 선택\n",
        "        data = data_sample[i][-1, :]  # 동일한 차원 축소 적용\n",
        "    else:\n",
        "        data = data_sample[i]\n",
        "\n",
        "    print(f\"Feature {i} SHAP value shape after slicing: {shap_value.shape}\")\n",
        "    #shap.plots.waterfall(shap.Explanation(values=shap_value,\n",
        "                                          #base_values=base_values_sample,\n",
        "                                          #data=data))\n",
        "\n"
      ],
      "metadata": {
        "id": "4sK6FSa6KaFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "f241aa05-0620-44ac-e544-8e4ed5ddbcaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0 SHAP value shape after slicing: (24, 8, 209)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8fbb408ca7aa>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 여러 차원을 가진 경우, 마지막 차원을 선택 (예시: [:, :] 선택)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mshap_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 마지막 차원 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 동일한 차원 축소 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 개별 샘플에 대한 Waterfall 플롯 그리기\n",
        "sample_index = 0  # 시각화할 샘플 인덱스\n",
        "shap_values_np = np.array(shap_values)\n",
        "\n",
        "# SHAP 값 시각화\n",
        "shap.plots.waterfall(shap.Explanation(values=shap_values_np[sample_index],\n",
        "                                      base_values=explainer.expected_value[sample_index],\n",
        "                                      data=inputs[0][sample_index].cpu().numpy()))"
      ],
      "metadata": {
        "id": "pj7FnyQgYx2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(data_sample.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "_sDyHfMoxCPP",
        "outputId": "88803c18-cbc7-4c38-c9ba-a7b91dadf872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-06179c13cca7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
        "\n",
        "print(shap_value.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "Y4NbnX1Hxxz7",
        "outputId": "a765c09d-b4d3-40d5-a562-93b90b0367d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 209, 2, 209)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;31m# find the right formatting function for the array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m     \u001b[0mformat_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_format_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_get_format_function\u001b[0;34m(data, **options)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mformatdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypeobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplexfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;34m'int'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIntegerFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         'float': lambda: FloatingFormat(\n\u001b[0m\u001b[1;32m    412\u001b[0m             data, precision, floatmode, suppress, sign, legacy=legacy),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, precision, floatmode, suppress_small, sign, legacy)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mfillFormat\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    988\u001b[0m                     for x in finite_vals)\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mint_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m113\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    988\u001b[0m                     for x in finite_vals)\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mint_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m113\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cf467d031382>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_array_repr_implementation\u001b[0;34m(arr, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m         lst = array2string(arr, max_line_width, precision, suppress_small,\n\u001b[0m\u001b[1;32m   1509\u001b[0m                            ', ', prefix, suffix=suffix)\n\u001b[1;32m   1510\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# show zero-length shape unless it is (0,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"[]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_array2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BAR 시각화"
      ],
      "metadata": {
        "id": "4hDnYXigKUEY"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}